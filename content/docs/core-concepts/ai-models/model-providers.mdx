---
title: Model Providers
description: Working with different AI model providers in elizaOS
---

# Model Providers

Model providers in elizaOS abstract the complexity of working with different AI services, allowing you to seamlessly switch between providers or use multiple providers simultaneously. Each provider implements a standardized interface while handling the specifics of their respective APIs.

## Overview

The model provider system enables:
- **Unified API**: Single interface for all AI models
- **Runtime switching**: Change providers without code modifications
- **Multi-provider support**: Use different providers for different model types
- **Automatic fallbacks**: Graceful degradation when providers are unavailable
- **Priority-based selection**: Control which provider is used for specific tasks

## Supported Model Providers

### OpenAI

The OpenAI provider offers access to GPT models, embeddings, and DALL-E image generation.

**Installation:**
```bash
npm install @elizaos/plugin-openai
```

**Configuration:**
```typescript
// Character file
{
  "name": "MyAgent",
  "plugins": ["@elizaos/plugin-openai"]
}

// Environment variables
OPENAI_API_KEY=your-api-key
```
```

**Environment Variables:**
```bash
OPENAI_API_KEY=sk-...
OPENAI_API_BASE=https://api.openai.com/v1  # Optional
```

**Supported Models:**
- **Text Generation**: GPT-4, GPT-4 Turbo, GPT-3.5-turbo
- **Embeddings**: text-embedding-3-large, text-embedding-3-small
- **Image Generation**: DALL-E 3, DALL-E 2
- **Audio**: Whisper (transcription), TTS (text-to-speech)

**Usage Example:**
```typescript
// Automatic model selection based on modelProvider
const response = await runtime.useModel(ModelType.TEXT_LARGE, {
  prompt: "Explain quantum computing",
  temperature: 0.7,
  maxTokens: 1000
});
```

### Anthropic (Claude)

The Anthropic provider provides access to Claude models known for their safety and reasoning capabilities.

**Installation:**
```bash
npm install @elizaos/plugin-anthropic
```

**Configuration:**
```typescript
{
  "name": "MyAgent",
  "plugins": ["@elizaos/plugin-anthropic"]
}

// Environment variables
ANTHROPIC_API_KEY=your-api-key
```
```

**Environment Variables:**
```bash
ANTHROPIC_API_KEY=sk-ant-...
```

**Supported Models:**
- **Text Generation**: Claude-3 Opus, Claude-3 Sonnet, Claude-3 Haiku
- **Vision**: Claude-3 Vision (image description)

**Usage Example:**
```typescript
const response = await runtime.useModel(ModelType.TEXT_LARGE, {
  prompt: "Analyze this code for potential issues",
  temperature: 0.3,
  maxTokens: 2000
});
```

### Google Gemini

The Google Gemini provider offers access to Google's multimodal AI models.

**Installation:**
```bash
npm install @elizaos/plugin-google-genai
```

**Configuration:**
```typescript
{
  "name": "MyAgent",
  "plugins": ["@elizaos/plugin-google-genai"]
}

// Environment variables
GOOGLE_GENERATIVE_AI_API_KEY=your-api-key
```
```

**Environment Variables:**
```bash
GOOGLE_GENERATIVE_AI_API_KEY=AI...
```

**Supported Models:**
- **Text Generation**: Gemini Pro, Gemini Flash
- **Embeddings**: text-embedding-004
- **Vision**: Gemini Pro Vision

**Usage Example:**
```typescript
const response = await runtime.useModel(ModelType.TEXT_LARGE, {
  prompt: "Write a creative story about AI",
  temperature: 0.8,
  maxTokens: 1500
});
```

### Ollama (Local Models)

The Ollama provider enables running open-source models locally.

**Installation:**
```bash
npm install @elizaos/plugin-ollama
```

**Prerequisites:**
1. Install Ollama: https://ollama.ai
2. Pull models: `ollama pull llama3.1`

**Configuration:**
```typescript
{
  "name": "MyAgent",
  "plugins": ["@elizaos/plugin-ollama"]
}

// Environment variables
OLLAMA_API_ENDPOINT=http://localhost:11434
OLLAMA_MODEL=llama3.1
```
```

**Environment Variables:**
```bash
OLLAMA_API_ENDPOINT=http://localhost:11434
OLLAMA_MODEL=llama3.1
```

**Supported Models:**
- **Text Generation**: Llama 3.1, Mistral, CodeLlama, Phi-3
- **Embeddings**: nomic-embed-text, all-minilm
- **Vision**: llava, moondream

**Usage Example:**
```typescript
const response = await runtime.useModel(ModelType.TEXT_LARGE, {
  prompt: "Explain machine learning concepts",
  temperature: 0.6,
  maxTokens: 1000
});
```

### Heurist AI

The Heurist provider offers decentralized AI inference.

**Installation:**
```bash
npm install @elizaos/plugin-heurist
```

**Configuration:**
```typescript
{
  "name": "MyAgent",
  "plugins": ["@elizaos/plugin-heurist"]
}

// Environment variables
HEURIST_API_KEY=your-api-key
```
```

### OpenRouter

The OpenRouter provider gives access to multiple models through a unified API.

**Installation:**
```bash
npm install @elizaos/plugin-openrouter
```

**Configuration:**
```typescript
{
  "name": "MyAgent",
  "plugins": ["@elizaos/plugin-openrouter"]
}

// Environment variables
OPENROUTER_API_KEY=your-api-key
```
```

**Environment Variables:**
```bash
OPENROUTER_API_KEY=sk-or-...
```

### Local AI

The Local AI provider enables running AI models completely offline.

**Installation:**
```bash
npm install @elizaos/plugin-local-ai
```

**Configuration:**
```typescript
{
  "name": "MyAgent",
  "plugins": ["@elizaos/plugin-local-ai"]
}

// Environment variables
USE_LOCAL_AI=true
```

**Usage Example:**
```typescript
const response = await runtime.useModel(ModelType.TEXT_LARGE, {
  prompt: "Generate a response using local AI",
  temperature: 0.7,
  maxTokens: 1000
});
```

## Provider Selection Strategies

### Primary Provider

The ElizaOS runtime automatically selects the best available provider based on plugin order and priority:

```typescript
{
  "name": "MyAgent",
  "plugins": ["@elizaos/plugin-openai"]  // Single provider
}
```

### Multi-Provider Setup

Use different providers for different capabilities:

```typescript
{
  "name": "MyAgent",
  "plugins": [
    "@elizaos/plugin-sql",        // Always first
    "@elizaos/plugin-anthropic",  // Text generation
    "@elizaos/plugin-openai",     // Embeddings and image generation
    "@elizaos/plugin-ollama"      // Local models for privacy
  ]
}
```

### Priority-Based Selection

Control provider selection through plugin priority:

```typescript
// Higher priority plugins are selected first
const plugins = [
  { name: '@elizaos/plugin-openai', priority: 1000 },
  { name: '@elizaos/plugin-anthropic', priority: 900 },
  { name: '@elizaos/plugin-ollama', priority: 500 }
];
```

### Conditional Provider Loading

Load providers based on environment variables:

```typescript
const getPlugins = () => {
  const plugins = ['@elizaos/plugin-sql'];
  
  // Add providers based on available API keys
  if (process.env.OPENAI_API_KEY) {
    plugins.push('@elizaos/plugin-openai');
  }
  if (process.env.ANTHROPIC_API_KEY) {
    plugins.push('@elizaos/plugin-anthropic');
  }
  if (process.env.OLLAMA_API_ENDPOINT) {
    plugins.push('@elizaos/plugin-ollama');
  }
  
  return plugins;
};

export const character = {
  name: 'MyAgent',
  plugins: getPlugins()
};
```

## Advanced Configuration

### Model-Specific Settings

Configure individual model parameters:

```typescript
{
  "name": "MyAgent",
  "modelProvider": "openai",
  "settings": {
    "modelConfig": {
      "temperature": 0.7,
      "maxOutputTokens": 2000,
      "frequency_penalty": 0.1,
      "presence_penalty": 0.1
    }
  }
}
```

### Custom Endpoints

Use custom API endpoints:

```typescript
{
  "name": "MyAgent",
  "modelProvider": "openai",
  "modelEndpointOverride": "https://custom-api.example.com/v1",
  "settings": {
    "secrets": {
      "OPENAI_API_KEY": "custom-key"
    }
  }
}
```

### Provider-Specific Configuration

Different providers may have unique configuration options:

```typescript
{
  "name": "MyAgent",
  "settings": {
    // OpenAI specific
    "openai": {
      "organization": "org-123",
      "project": "proj-456"
    },
    
    // Anthropic specific
    "anthropic": {
      "version": "2023-06-01"
    },
    
    // Ollama specific
    "ollama": {
      "host": "localhost:11434",
      "keep_alive": "5m"
    }
  }
}
```

## Runtime Provider Management

### Dynamic Provider Selection

Select providers programmatically:

```typescript
// In your action or provider
const response = await runtime.useModel(ModelType.TEXT_LARGE, {
  prompt: "Generate response",
  temperature: 0.7
}, 'anthropic'); // Force specific provider
```

### Provider Availability Check

Check if a provider is available:

```typescript
const hasEmbeddings = runtime.getModel(ModelType.TEXT_EMBEDDING);
if (hasEmbeddings) {
  const embedding = await runtime.useModel(ModelType.TEXT_EMBEDDING, {
    text: "Sample text"
  });
}
```

### Fallback Mechanisms

Implement automatic fallbacks:

```typescript
async function generateWithFallback(runtime, prompt) {
  const providers = ['openai', 'anthropic', 'ollama'];
  
  for (const provider of providers) {
    try {
      return await runtime.useModel(ModelType.TEXT_LARGE, {
        prompt,
        temperature: 0.7
      }, provider);
    } catch (error) {
      console.warn(`Provider ${provider} failed:`, error.message);
      continue;
    }
  }
  
  throw new Error('All providers failed');
}
```

## Provider Comparison

| Provider | Strengths | Best For | Limitations |
|----------|-----------|----------|-------------|
| **OpenAI** | Most advanced models, comprehensive API | Production apps, complex reasoning | Cost, API dependency |
| **Anthropic** | Safety-focused, excellent reasoning | Content analysis, code review | Limited model variety |
| **Google Gemini** | Fast, multimodal, competitive pricing | Real-time apps, mixed media | Newer ecosystem |
| **Ollama** | Privacy, no API costs, customizable | Local deployment, privacy-sensitive | Requires local resources |
| **Heurist** | Decentralized, cost-effective | Blockchain apps, distributed systems | Limited model selection |
| **OpenRouter** | Access to many models, unified API | Model comparison, flexibility | Third-party dependency |
| **Local AI** | Complete privacy, no API calls | Offline deployment, sensitive data | Setup complexity, resource intensive |

## Cost Optimization

### Model Size Selection

Choose appropriate model sizes:

```typescript
// Use smaller models for simple tasks
const simpleResponse = await runtime.useModel(ModelType.TEXT_SMALL, {
  prompt: "Summarize in one sentence"
});

// Use larger models for complex reasoning
const complexResponse = await runtime.useModel(ModelType.TEXT_LARGE, {
  prompt: "Analyze this complex problem"
});
```

### Provider Cost Comparison

Monitor usage across providers:

```typescript
const costTracker = {
  openai: { requests: 0, tokens: 0 },
  anthropic: { requests: 0, tokens: 0 },
  ollama: { requests: 0, tokens: 0 }
};

// Track usage in your application
function trackUsage(provider, tokens) {
  costTracker[provider].requests++;
  costTracker[provider].tokens += tokens;
}
```

## Best Practices

### 1. Provider Redundancy

Always have backup providers:

```typescript
const primaryProviders = ['openai', 'anthropic'];
const fallbackProviders = ['ollama', 'local-ai'];
```

### 2. Environment-Specific Configuration

Use different providers for different environments:

```typescript
const getProviderConfig = () => {
  if (process.env.NODE_ENV === 'production') {
    return {
      modelProvider: 'openai',
      plugins: ['@elizaos/plugin-openai', '@elizaos/plugin-anthropic']
    };
  } else {
    return {
      modelProvider: 'ollama',
      plugins: ['@elizaos/plugin-ollama']
    };
  }
};
```

### 3. Model Type Specialization

Use different providers for different model types:

```typescript
// OpenAI for embeddings
const embedding = await runtime.useModel(ModelType.TEXT_EMBEDDING, {
  text: "Sample text"
}, 'openai');

// Anthropic for reasoning
const reasoning = await runtime.useModel(ModelType.TEXT_LARGE, {
  prompt: "Analyze this data"
}, 'anthropic');

// Ollama for privacy-sensitive tasks
const privateTask = await runtime.useModel(ModelType.TEXT_LARGE, {
  prompt: "Process confidential data"
}, 'ollama');
```

### 4. Configuration Validation

Validate provider configurations on startup:

```typescript
export const validateProviderConfig = (character) => {
  const requiredVars = {
    openai: ['OPENAI_API_KEY'],
    anthropic: ['ANTHROPIC_API_KEY'],
    google: ['GOOGLE_GENERATIVE_AI_API_KEY']
  };
  
  const provider = character.modelProvider;
  const required = requiredVars[provider];
  
  if (required) {
    for (const envVar of required) {
      if (!process.env[envVar]) {
        throw new Error(`${envVar} is required for ${provider} provider`);
      }
    }
  }
};
```

## Troubleshooting

### Common Issues

1. **Provider not available:**
   - Check plugin installation
   - Verify API keys
   - Ensure provider is in plugins array

2. **Model not supported:**
   - Check provider capabilities
   - Verify model type constants
   - Check provider documentation

3. **API key issues:**
   - Verify environment variables
   - Check API key format
   - Confirm API key permissions

### Debug Information

Enable debug logging:

```typescript
// Set environment variable
LOG_LEVEL=debug

// Or in code
runtime.logger.debug('Provider selection:', {
  modelType: ModelType.TEXT_LARGE,
  availableProviders: Object.keys(runtime.models)
});
```

The model provider system in elizaOS offers flexibility and reliability for AI model integration. By understanding the different providers and their capabilities, you can build robust agents that leverage the best models for each task while maintaining fallback options for reliability.